<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta charset="utf-8">
<meta name="generator" content="ReSpec 34.1.4">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<style>
.issue-label{text-transform:initial}
.warning>p:first-child{margin-top:0}
.warning{padding:.5em;border-left-width:.5em;border-left-style:solid}
span.warning{padding:.1em .5em .15em}
.issue.closed span.issue-number{text-decoration:line-through}
.issue.closed span.issue-number::after{content:" (Closed)";font-size:smaller}
.warning{border-color:#f11;border-width:.2em;border-style:solid;background:#fbe9e9}
.warning-title:before{content:"⚠";font-size:1.3em;float:left;padding-right:.3em;margin-top:-.3em}
li.task-list-item{list-style:none}
input.task-list-item-checkbox{margin:0 .35em .25em -1.6em;vertical-align:middle}
.issue a.respec-gh-label{padding:5px;margin:0 2px 0 2px;font-size:10px;text-transform:none;text-decoration:none;font-weight:700;border-radius:4px;position:relative;bottom:2px;border:none;display:inline-block}
</style>
<style>
dfn{cursor:pointer}
.dfn-panel{position:absolute;z-index:35;min-width:300px;max-width:500px;padding:.5em .75em;margin-top:.6em;font-family:"Helvetica Neue",sans-serif;font-size:small;background:#fff;color:#000;box-shadow:0 1em 3em -.4em rgba(0,0,0,.3),0 0 1px 1px rgba(0,0,0,.05);border-radius:2px}
.dfn-panel:not(.docked)>.caret{position:absolute;top:-9px}
.dfn-panel:not(.docked)>.caret::after,.dfn-panel:not(.docked)>.caret::before{content:"";position:absolute;border:10px solid transparent;border-top:0;border-bottom:10px solid #fff;top:0}
.dfn-panel:not(.docked)>.caret::before{border-bottom:9px solid #a2a9b1}
.dfn-panel *{margin:0}
.dfn-panel b{display:block;color:#000;margin-top:.25em}
.dfn-panel ul a[href]{color:#333}
.dfn-panel>div{display:flex}
.dfn-panel a.self-link{font-weight:700;margin-right:auto}
.dfn-panel .marker{padding:.1em;margin-left:.5em;border-radius:.2em;text-align:center;white-space:nowrap;font-size:90%;color:#040b1c}
.dfn-panel .marker.dfn-exported{background:#d1edfd;box-shadow:0 0 0 .125em #1ca5f940}
.dfn-panel .marker.idl-block{background:#8ccbf2;box-shadow:0 0 0 .125em #0670b161}
.dfn-panel a:not(:hover){text-decoration:none!important;border-bottom:none!important}
.dfn-panel a[href]:hover{border-bottom-width:1px}
.dfn-panel ul{padding:0}
.dfn-panel li{margin-left:1em}
.dfn-panel.docked{position:fixed;left:.5em;top:unset;bottom:2em;margin:0 auto;max-width:calc(100vw - .75em * 2 - .5em - .2em * 2);max-height:30vh;overflow:auto}
</style>
	  
	  
<title>Synchronization Accessibility User Requirements</title>
	  		
		
		
	
<style id="respec-mainstyle">
@keyframes pop{
0%{transform:scale(1,1)}
25%{transform:scale(1.25,1.25);opacity:.75}
100%{transform:scale(1,1)}
}
:is(h1,h2,h3,h4,h5,h6,a) abbr{border:none}
dfn{font-weight:700}
a.internalDFN{color:inherit;border-bottom:1px solid #99c;text-decoration:none}
a.externalDFN{color:inherit;border-bottom:1px dotted #ccc;text-decoration:none}
a.bibref{text-decoration:none}
.respec-offending-element:target{animation:pop .25s ease-in-out 0s 1}
.respec-offending-element,a[href].respec-offending-element{text-decoration:red wavy underline}
@supports not (text-decoration:red wavy underline){
.respec-offending-element:not(pre){display:inline-block}
.respec-offending-element{background:url(data:image/gif;base64,R0lGODdhBAADAPEAANv///8AAP///wAAACwAAAAABAADAEACBZQjmIAFADs=) bottom repeat-x}
}
#references :target{background:#eaf3ff;animation:pop .4s ease-in-out 0s 1}
cite .bibref{font-style:normal}
a[href].orcid{padding-left:4px;padding-right:4px}
a[href].orcid>svg{margin-bottom:-2px}
.toc a,.tof a{text-decoration:none}
a .figno,a .secno{color:#000}
ol.tof,ul.tof{list-style:none outside none}
.caption{margin-top:.5em;font-style:italic}
table.simple{border-spacing:0;border-collapse:collapse;border-bottom:3px solid #005a9c}
.simple th{background:#005a9c;color:#fff;padding:3px 5px;text-align:left}
.simple th a{color:#fff;padding:3px 5px;text-align:left}
.simple th[scope=row]{background:inherit;color:inherit;border-top:1px solid #ddd}
.simple td{padding:3px 10px;border-top:1px solid #ddd}
.simple tr:nth-child(even){background:#f0f6ff}
.section dd>p:first-child{margin-top:0}
.section dd>p:last-child{margin-bottom:0}
.section dd{margin-bottom:1em}
.section dl.attrs dd,.section dl.eldef dd{margin-bottom:0}
#issue-summary>ul{column-count:2}
#issue-summary li{list-style:none;display:inline-block}
details.respec-tests-details{margin-left:1em;display:inline-block;vertical-align:top}
details.respec-tests-details>*{padding-right:2em}
details.respec-tests-details[open]{z-index:999999;position:absolute;border:thin solid #cad3e2;border-radius:.3em;background-color:#fff;padding-bottom:.5em}
details.respec-tests-details[open]>summary{border-bottom:thin solid #cad3e2;padding-left:1em;margin-bottom:1em;line-height:2em}
details.respec-tests-details>ul{width:100%;margin-top:-.3em}
details.respec-tests-details>li{padding-left:1em}
.self-link:hover{opacity:1;text-decoration:none;background-color:transparent}
aside.example .marker>a.self-link{color:inherit}
.header-wrapper{display:flex;align-items:baseline}
:is(h2,h3,h4,h5,h6):not(#toc>h2,#abstract>h2,#sotd>h2,.head>h2){position:relative;left:-.5em}
:is(h2,h3,h4,h5,h6):not(#toch2)+a.self-link{color:inherit;order:-1;position:relative;left:-1.1em;font-size:1rem;opacity:.5}
:is(h2,h3,h4,h5,h6)+a.self-link::before{content:"§";text-decoration:none;color:var(--heading-text)}
:is(h2,h3)+a.self-link{top:-.2em}
:is(h4,h5,h6)+a.self-link::before{color:#000}
@media (max-width:767px){
dd{margin-left:0}
}
@media print{
.removeOnSave{display:none}
}
</style>
<meta name="description" content="This document summarizes relevant research, then outlines accessibility-related user needs and associated requirements for the synchronization of audio and visual media. The scope of the discussion includes synchronization of accessibility-related components of multimedia, such as captions, sign language interpretation, and descriptions. The requirements identified herein are also applicable to multimedia content in general, as well as real-time communication applications and media occurring in immersive environments.">
<link rel="canonical" href="https://www.w3.org/TR/saur/">
<style>
var{position:relative;cursor:pointer}
var[data-type]::after,var[data-type]::before{position:absolute;left:50%;top:-6px;opacity:0;transition:opacity .4s;pointer-events:none}
var[data-type]::before{content:"";transform:translateX(-50%);border-width:4px 6px 0 6px;border-style:solid;border-color:transparent;border-top-color:#000}
var[data-type]::after{content:attr(data-type);transform:translateX(-50%) translateY(-100%);background:#000;text-align:center;font-family:"Dank Mono","Fira Code",monospace;font-style:normal;padding:6px;border-radius:3px;color:#daca88;text-indent:0;font-weight:400}
var[data-type]:hover::after,var[data-type]:hover::before{opacity:1}
</style>
<script id="initialUserConfig" type="application/json">{
  "trace": true,
  "useExperimentalStyles": true,
  "doRDFa": "1.1",
  "includePermalinks": true,
  "permalinkEdge": true,
  "permalinkHide": false,
  "noRecTrack": true,
  "tocIntroductory": true,
  "specStatus": "NOTE",
  "diffTool": "http://www.aptest.com/standards/htmldiff/htmldiff.pl",
  "shortName": "saur",
  "copyrightStart": "2022",
  "license": "w3c-software-doc",
  "editors": [
    {
      "name": "Steve Noble",
      "company": "Pearson",
      "mailto": "steve.noble@pearson.com",
      "w3cid": 81519,
      "url": "mailto:steve.noble@pearson.com"
    },
    {
      "name": "Jason White",
      "company": "Educational Testing Service",
      "mailto": "jjwhite@ets.org",
      "w3cid": 74028,
      "url": "mailto:jjwhite@ets.org"
    },
    {
      "name": "Scott Hollier",
      "company": "Invited Expert",
      "mailto": "scott@hollier.info",
      "w3cid": 43274,
      "url": "mailto:scott@hollier.info"
    },
    {
      "name": "Janina Sajka",
      "company": "Invited Expert",
      "mailto": "janina@rednote.net",
      "w3cid": 33688,
      "url": "mailto:janina@rednote.net"
    },
    {
      "name": "Joshue O'Connor",
      "company": "Invited Expert",
      "mailto": "josh@interaccess.ie",
      "w3cid": 41218,
      "url": "mailto:josh@interaccess.ie"
    }
  ],
  "group": "apa",
  "github": "w3c/saur",
  "maxTocLevel": 4,
  "publishISODate": "2023-06-28T00:00:00.000Z",
  "generatedSubtitle": "W3C Group Note 28 June 2023"
}</script>
<link rel="stylesheet" href="https://www.w3.org/StyleSheets/TR/2021/W3C-NOTE"></head>
	<body class="h-entry informative" data-new-gr-c-s-check-loaded="14.1113.0" data-gr-ext-installed=""><div class="head">
    <p class="logos"><a class="logo" href="https://www.w3.org/"><img crossorigin="" alt="W3C" height="48" src="https://www.w3.org/StyleSheets/TR/2021/logos/W3C" width="72">
  </a></p>
    <h1 id="title" class="title">Synchronization Accessibility User Requirements</h1> 
    <p id="w3c-state"><a href="https://www.w3.org/standards/types#NOTE">W3C Group Note</a> <time class="dt-published" datetime="2023-06-28">28 June 2023</time></p>
    <details open="">
      <summary>More details about this document</summary>
      <dl>
        <dt>This version:</dt><dd>
                <a class="u-url" href="https://www.w3.org/TR/2023/NOTE-saur-20230628/">https://www.w3.org/TR/2023/NOTE-saur-20230628/</a>
              </dd>
        <dt>Latest published version:</dt><dd>
                <a href="https://www.w3.org/TR/saur/">https://www.w3.org/TR/saur/</a>
              </dd>
        <dt>Latest editor's draft:</dt><dd><a href="https://w3c.github.io/saur/">https://w3c.github.io/saur/</a></dd>
        <dt>History:</dt><dd>
                    <a href="https://www.w3.org/standards/history/saur/">https://www.w3.org/standards/history/saur/</a>
                  </dd><dd>
                    <a href="https://github.com/w3c/saur/commits/">Commit history</a>
                  </dd>
        
        
        
        
        
        <dt>Editors:</dt><dd class="editor p-author h-card vcard" data-editor-id="81519">
    <a class="ed_mailto u-email email p-name" href="mailto:steve.noble@pearson.com">Steve Noble</a> (<span class="p-org org h-org">Pearson</span>)
  </dd><dd class="editor p-author h-card vcard" data-editor-id="74028">
    <a class="ed_mailto u-email email p-name" href="mailto:jjwhite@ets.org">Jason White</a> (<span class="p-org org h-org">Educational Testing Service</span>)
  </dd><dd class="editor p-author h-card vcard" data-editor-id="43274">
    <a class="ed_mailto u-email email p-name" href="mailto:scott@hollier.info">Scott Hollier</a> (<span class="p-org org h-org">Invited Expert</span>)
  </dd><dd class="editor p-author h-card vcard" data-editor-id="33688">
    <a class="ed_mailto u-email email p-name" href="mailto:janina@rednote.net">Janina Sajka</a> (<span class="p-org org h-org">Invited Expert</span>)
  </dd><dd class="editor p-author h-card vcard" data-editor-id="41218">
    <a class="ed_mailto u-email email p-name" href="mailto:josh@interaccess.ie">Joshue O'Connor</a> (<span class="p-org org h-org">Invited Expert</span>)
  </dd>
        
        
        <dt>Feedback:</dt><dd>
        <a href="https://github.com/w3c/saur/">GitHub w3c/saur</a>
        (<a href="https://github.com/w3c/saur/pulls/">pull requests</a>,
        <a href="https://github.com/w3c/saur/issues/new/choose">new issue</a>,
        <a href="https://github.com/w3c/saur/issues/">open issues</a>)
      </dd>
        
        
      </dl>
    </details>
    
    
    <p class="copyright">
    <a href="https://www.w3.org/Consortium/Legal/ipr-notice#Copyright">Copyright</a>
    ©
    2022-2023
    
    <a href="https://www.w3.org/">World Wide Web Consortium</a>.
    <abbr title="World Wide Web Consortium">W3C</abbr><sup>®</sup>
    <a href="https://www.w3.org/Consortium/Legal/ipr-notice#Legal_Disclaimer">liability</a>,
    <a href="https://www.w3.org/Consortium/Legal/ipr-notice#W3C_Trademarks">trademark</a> and
    <a rel="license" href="https://www.w3.org/Consortium/Legal/2023/software-license" title="W3C Software and Document Notice and License">permissive document license</a> rules apply.
  </p>
    <hr title="Separator for header">
  </div>
	  <section id="abstract" class="introductory"><h2>Abstract</h2>
	  	
	    <p>This document summarizes relevant research, then outlines accessibility-related user needs and associated requirements for the synchronization of audio and visual media. The scope of the discussion includes synchronization of accessibility-related components of multimedia, such as captions, sign language interpretation, and descriptions. The requirements identified herein are also applicable to multimedia content in general, as well as real-time communication applications and media occurring in immersive environments.</p>
	    <p>The purpose of this document is to identify and to characterize synchronization-related needs. It does not constitute normative guidance. It may, nevertheless, influence the further development of <abbr title="World Wide Web Consortium">W3C</abbr> specifications, including accessibility guidelines and media-related technologies. It may also be applied to the development of multimedia content and applications to enhance accessibility.</p>
	  </section>
	  <section id="sotd" class="introductory"><h2>Status of This Document</h2><p><em>This section describes the status of this
      document at the time of its publication. A list of current <abbr title="World Wide Web Consortium">W3C</abbr>
      publications and the latest revision of this technical report can be found
      in the <a href="https://www.w3.org/TR/"><abbr title="World Wide Web Consortium">W3C</abbr> technical reports index</a> at
      https://www.w3.org/TR/.</em></p>
	  <p>
    This document was published by the <a href="https://www.w3.org/groups/wg/apa">Accessible Platform Architectures Working Group</a> as
    a Group Note using the
        <a href="https://www.w3.org/2021/Process-20211102/#recs-and-notes">Note track</a>. 
  </p><p>This Group Note is endorsed by
        the <a href="https://www.w3.org/groups/wg/apa">Accessible Platform Architectures Working Group</a>, but is not endorsed by
        <abbr title="World Wide Web Consortium">W3C</abbr> itself nor its
        Members. </p><p>
    This is a draft document and may be updated, replaced or obsoleted by other
    documents at any time. It is inappropriate to cite this document as other
    than work in progress.
    
  </p><p data-deliverer="83907">
    
        The
        <a href="https://www.w3.org/Consortium/Patent-Policy/"><abbr title="World Wide Web Consortium">W3C</abbr> Patent
          Policy</a>
        does not carry any licensing requirements or commitments on this
        document.
      
    
  </p><p>
                  This document is governed by the
                  <a id="w3c_process_revision" href="https://www.w3.org/2021/Process-20211102/">2 November 2021 <abbr title="World Wide Web Consortium">W3C</abbr> Process Document</a>.
                </p></section><nav id="toc"><h2 class="introductory" id="table-of-contents">Table of Contents</h2><ol class="toc"><li class="tocline"><a class="tocxref" href="#abstract">Abstract</a></li><li class="tocline"><a class="tocxref" href="#sotd">Status of This Document</a></li><li class="tocline"><a class="tocxref" href="#introduction"><bdi class="secno">1. </bdi>Introduction</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#media-synchronization"><bdi class="secno">1.1 </bdi>Media Synchronization</a></li><li class="tocline"><a class="tocxref" href="#media-synchronization-and-accessibility"><bdi class="secno">1.2 </bdi>Media Synchronization and Accessibility</a></li><li class="tocline"><a class="tocxref" href="#associated-publications"><bdi class="secno">1.3 </bdi>Associated Publications</a></li></ol></li><li class="tocline"><a class="tocxref" href="#issues-and-opportunities-identified-in-the-literature"><bdi class="secno">2. </bdi>Issues and Opportunities Identified in the Literature</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#lip-reading-use-case-synchronization"><bdi class="secno">2.1 </bdi>Lip Reading Use Case Synchronization</a></li><li class="tocline"><a class="tocxref" href="#caption-synchronization"><bdi class="secno">2.2 </bdi>Caption Synchronization</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#caption-rate"><bdi class="secno">2.2.1 </bdi>Caption Rate</a></li><li class="tocline"><a class="tocxref" href="#captions-in-live-media"><bdi class="secno">2.2.2 </bdi>Captions in Live Media</a></li><li class="tocline"><a class="tocxref" href="#caption-synchronization-thresholds"><bdi class="secno">2.2.3 </bdi>Caption Synchronization Thresholds</a></li></ol></li><li class="tocline"><a class="tocxref" href="#sign-language-interpretation-synchronization"><bdi class="secno">2.3 </bdi>Sign Language Interpretation Synchronization</a></li><li class="tocline"><a class="tocxref" href="#video-description-synchronization"><bdi class="secno">2.4 </bdi>Video Description Synchronization</a></li><li class="tocline"><a class="tocxref" href="#xr-environment-synchronization"><bdi class="secno">2.5 </bdi>XR Environment Synchronization</a></li></ol></li><li class="tocline"><a class="tocxref" href="#acknowledgements"><bdi class="secno">A. </bdi>Acknowledgements</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#enabling-funders"><bdi class="secno">A.1 </bdi>Enabling Funders</a></li></ol></li><li class="tocline"><a class="tocxref" href="#references"><bdi class="secno">B. </bdi>References</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#informative-references"><bdi class="secno">B.1 </bdi>Informative references</a></li></ol></li></ol></nav>
	  <section id="introduction"><div class="header-wrapper"><h2 id="x1-introduction"><bdi class="secno">1. </bdi>Introduction</h2><a class="self-link" href="#introduction" aria-label="Permalink for Section 1."></a></div>
	    
	    <section id="media-synchronization"><div class="header-wrapper"><h3 id="x1-1-media-synchronization"><bdi class="secno">1.1 </bdi>Media Synchronization</h3><a class="self-link" href="#media-synchronization" aria-label="Permalink for Section 1.1"></a></div>
	      
	      <p>In accessible multimedia content, a variety of resources may be presented concurrently. These resources can include a video track, an audio track, captions, video descriptions, and sign language interpretation of the audio track. To ensure equality of access for all users, including those with a variety of disabilities and associated needs, these concurrent resources should be appropriately synchronized. For example, adequate synchronization of the audio and video tracks is necessary to support users who are hard of hearing, and who rely on lip reading to understand spoken content. Users who have difficulty hearing for situational reasons (e.g., due to a noisy environment) also benefit.</p>
	      <p>This document addresses the question of what qualifies as sufficient synchronization of the different media resources that may be used in accessible content. The considerations that bear on this question are different depending on the resources involved (audio and video tracks, captions, sign language interpretation, etc.). Likewise, the applicable constraints vary according to whether the multimedia content is presented in real time (as in a video conference or a live event), or prerecorded.</p>
	      <p>Adequate synchronization benefits all users. Consequently, the research surveyed in this document is of general importance to the quality of multimedia for all user populations. It is especially significant, however, to users with disabilities who need alternative media resources, such as captions, descriptions, and sign language interpretation.</p>
	    </section>
	    <section id="media-synchronization-and-accessibility"><div class="header-wrapper"><h3 id="x1-2-media-synchronization-and-accessibility"><bdi class="secno">1.2 </bdi>Media Synchronization and Accessibility</h3><a class="self-link" href="#media-synchronization-and-accessibility" aria-label="Permalink for Section 1.2"></a></div>
	      
	      <p>The adequacy of media synchronization can significantly affect the accessibility of content. For example, for a person who uses captions to follow the progression of a video successfully, a correspondence should be maintained between the captions and the visual track (both of which the user is watching concurrently). This can be accomplished by limiting the delay between the spoken dialogue and the presentation of the captions. The issue of what delay should be regarded as acceptable in such a case is addressed in this document with respect to a variety of media resources.</p>
	      <p>More precisely, one media track can be ahead of or behind another media track by a specific time interval. The problem of adequate synchronization can be understood as that of defining the appropriate tolerances for the relationship between different kinds of media resources, such as audio and video tracks. The purpose of limiting the acceptable time window is to facilitate comprehension of the material. Insufficient synchronization leads to a corresponding loss in comprehension.</p>
	      <p>Issues of media synchronization are relevant to multiple aspects of Web technology. These aspects include the design and implementation of Web standards for media synchronization (e.g., Timed Text), the authoring tools with which accessible multimedia content is created, and the Web applications through which it is presented to the user. This document can serve as a point of reference for the development of each of these technologies. It can also inform the further evolution of standards for Web accessibility, and indeed for multimedia accessibility in general.</p>
	      <div class="note" role="note" id="issue-container-generatedID"><div role="heading" class="note-title marker" id="h-note" aria-level="4"><span>Note</span></div><p class="">For synchronized media content to be effectively usable, there are quality conditions that need to be met. For instance, the frame rate of video must be adequate; otherwise, lip reading becomes impracticable. See <cite>RTC Accessibility User Requirements</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-raur" title="RTC Accessibility User Requirements">raur</a></cite>], section 5.1, which notes applicable technical standards for the minimum frame rate.</p></div>
	    </section>
	    <section id="associated-publications"><div class="header-wrapper"><h3 id="x1-3-associated-publications"><bdi class="secno">1.3 </bdi>Associated Publications</h3><a class="self-link" href="#associated-publications" aria-label="Permalink for Section 1.3"></a></div>
	      
	      <p>This document is closely related to other publications developed by the <abbr title="World Wide Web Consortium">W3C</abbr>'s Web Accessibility Initiative. Normative guidance concerning the accessibility of multimedia is given in <cite>Web Content Accessibility Guidelines (WCAG)</cite> 2.1 [<cite><a class="bibref" data-link-type="biblio" href="#bib-wcag21" title="Web Content Accessibility Guidelines (WCAG) 2.1">wcag21</a></cite>]. Detailed, non-normative guidance to the accessibility-related aspects of multimedia content is presented in the <cite>Media Accessibility User Requirements (MAUR)</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-media-accessibility-reqs" title="Media Accessibility User Requirements">media-accessibility-reqs</a></cite>], a document that identifies users' needs and associated solutions.</p>
	      <p>Synchronized media can occur in immersive environments, including virtual reality and augmented reality. The <cite>XR Accessibility User Requirements (XAUR)</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-xaur" title="XR Accessibility User Requirements">xaur</a></cite>] should be consulted for guidance concerning the accessibility of these technologies. Similarly, synchronized media can arise in real-time communication applications, such as remote meeting environments. The accessibility-related user needs and associated system requirements applicable to these applications are considered in <cite>RTC Accessibility User Requirements (RAUR)</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-raur" title="RTC Accessibility User Requirements">raur</a></cite>]. The present document should be regarded as complementing each of these publications by examining a specific aspect of media quality and accessibility.</p>
	    </section>
	  </section>
<section id="issues-and-opportunities-identified-in-the-literature"><div class="header-wrapper"><h2 id="x2-issues-and-opportunities-identified-in-the-literature"><bdi class="secno">2. </bdi>Issues and Opportunities Identified in the Literature</h2><a class="self-link" href="#issues-and-opportunities-identified-in-the-literature" aria-label="Permalink for Section 2."></a></div>

<section id="lip-reading-use-case-synchronization"><div class="header-wrapper"><h3 id="x2-1-lip-reading-use-case-synchronization"><bdi class="secno">2.1 </bdi>Lip Reading Use Case Synchronization</h3><a class="self-link" href="#lip-reading-use-case-synchronization" aria-label="Permalink for Section 2.1"></a></div>

<p>Research in the field of human speech perception underscores the fact that speech perception is routinely bimodal in nature, and depends not only on acoustic cues in human speech but also on visual cues such as lip movements and facial expressions. Due to this bimodality in speech perception, audio-visual interaction becomes an important design factor for multimodal communication systems, such as video telephony and video conferencing [<cite><a class="bibref" data-link-type="biblio" href="#bib-integration-multi" title="Audio-visual integration in multimodal communication">integration-multi</a></cite>].</p>
<p>It has been observed that humans use their sight to assist in aural communication. This has been found to be especially true in helping to separate speech from background noise by supplying a supplemental visual information source, which is useful when the listener has trouble comprehending the acoustic speech. Past research has shown that in such situations, even people who are not hard of hearing depend upon such visual cues to some extent [<cite><a class="bibref" data-link-type="biblio" href="#bib-lip-speech" title="Lipreading and audio-visual speech perception">lip-speech</a></cite>]. Access to robust visual speech information has been shown to lead to significant improvement in speech recognition in noisy environments. For instance, one study found that when only acoustic access to speech was available, auditory recognition was near 100% at 0 dB of signal-to-noise ratio (SNR) but fell to under 20% at minus 30 dB SNR. However, this study found that when visual access to the speaker was included, recognition only dropped from 100% to 90% over the same range [<cite><a class="bibref" data-link-type="biblio" href="#bib-speech-intel-noise" title="Visual contribution to speech intelligibility in noise">speech-intel-noise</a></cite>]. More recent studies have shown that when sighted people are attempting to listen to speech in high noise audiovisual samples, the result is greater visual fixations on the mouth of the speaker [<cite><a class="bibref" data-link-type="biblio" href="#bib-gaze-patterns" title="Gaze patterns and audiovisual speech enhancement">gaze-patterns</a></cite>] and stronger synchronizations between the auditory and visual motion/motor brain regions [<cite><a class="bibref" data-link-type="biblio" href="#bib-neural-synchrony" title="Enhanced neural synchrony between left auditory and premotor cortex is associated with successful phonetic categorization.">neural-synchrony</a></cite>].</p>
<p>A similar reliance on visual cues to help decode speech may also be at work in other instances where volume of the speaker's voice begins to degrade, such as while listening to a lecture in a large hall. Due to the fact that light travels at a much higher speed than sound, in a face-to-face setting a person will see a speaker’s lips and facial gestures sooner than the sound of the speaker’s voice arrives. In a normal in-person conversation this difference is negligible. However, as the distance increases, such as a student listening to an instructor in the classroom, this time lag will increase. For instance, at 22 feet, this difference is roughly 20 ms. At the same time, the listener’s perceived volume of a speakers voice drops with the distance traveled, which means the listener will rely more on visual cues. Indeed, experimental research has demonstrated that the ability to comprehend speech at increasing distances is improved when both audio and visual speech is available to the listener [<cite><a class="bibref" data-link-type="biblio" href="#bib-distance-effects" title="Effects of distance on visual and audiovisual speech recognition">distance-effects</a></cite>]. Such findings suggest that robust synchronized video along with the audio of speakers in virtual environments are likely to increase speech comprehension for hard of hearing listeners.</p>
<p>One important concern in audiovisual integration of speech audio and visual information is how closely these events are synchronized. Given that the observable facial movement for phoneme production can precede acoustic information by 100–200 ms, the temporal order of both the sensory input and electrophysiological effects suggests that visual speech information may provide predictions about upcoming auditory input. This fact likely explains why research has found that test subjects are less likely to notice minor auditory lags in audiovisual presentation of human speech than when the audio signal arrives first [<cite><a class="bibref" data-link-type="biblio" href="#bib-prediction-const" title="Prediction and constraint in audiovisual speech perception">prediction-const</a></cite>]. As a result, several standards bodied have attempted to set synchronization specifications for audiovisual broadcasting which typically provide a +/- threshold where audio lag is much less restrictive than video lag. Typically, these thresholds are more restrictive for higher quality signals, such as those for digital high definition television broadcasting. Case in point, the recognized industry standard adopted by the ATSC Implementation Subcommittee, the DSL Forum, and the ITU-T Recommendation G.1080, all include an audio/video delay threshold between plus 15 ms and minus 45 ms [<cite><a class="bibref" data-link-type="biblio" href="#bib-simul-trans-video" title="Assessing the importance of audio/video synchronization for simultaneous translation of video sequences">simul-trans-video</a></cite>]. This means that having the audio arrive up to 45 ms after the video is considered acceptable, but having the audio signal arrive more than 15 ms before the video is objectionable. Consistently with this approach, the EN 301 549 information and communication technology public procurement standard [<cite><a class="bibref" data-link-type="biblio" href="#bib-en-301-549" title="EN 301 549 v3.2.1: Harmonised European Standard - Accessibility requirements for ICT products and services">en-301-549</a></cite>] specifies a maximum time difference of 100 ms between the audio and the video, noting that the decline in intelligibility is greater if the audio is ahead of, rather than behind, the video track.</p>
<p>However, it is important to note that most audiovisual media in everyday life does not meet the capabilities of high definition television. Further, most experimental studies which have attempted to examine issues around lip video synchronization with audio have been conducted with standard video recording capabilities which are typically limited to a frame rate of 25 frames per second (fps). At 25 fps, there will be one frame every 40 ms, and as a result it becomes impossible to test synchronization errors below this time threshold [<cite><a class="bibref" data-link-type="biblio" href="#bib-multimodal-speech" title="Multimodal speech recognition: increasing accuracy using high speed video data">multimodal-speech</a></cite>]. Studies using high quality audiovisual content at much faster frame rates have shown that lip synchronization mismatch of 20 ms or less is imperceptible [<cite><a class="bibref" data-link-type="biblio" href="#bib-lip-sync-video" title="Lip Synchronization in Video Conferencing">lip-sync-video</a></cite>]. However, studies conducted on speech intelligibility when audio quality is degraded in such a way as to simulate age-related hearing loss have shown that when the audio signal leads the video, intelligibility declines appreciably for even the shortest asynchrony of 40 ms, but when the video signal leads the audio, intelligibility remains relatively stable for onset asynchronies up to 160 - 200 ms [<cite><a class="bibref" data-link-type="biblio" href="#bib-speech-intel" title="Speech intelligibility derived from asynchronous processing of auditory-visual information">speech-intel</a></cite>]. These findings suggest that, from an accessibility perspective, the audio signal should not be ahead of the video by more than 40 ms, and the video should not be ahead of the audio by more than 160 ms. However, less than 160 ms offset is desirable due to the fact that this much of a delay would be detectable and potentially objectionable to a percentage of the population, even though it would not present an accessibility barrier as such.</p>
</section>
<section id="caption-synchronization"><div class="header-wrapper"><h3 id="x2-2-caption-synchronization"><bdi class="secno">2.2 </bdi>Caption Synchronization</h3><a class="self-link" href="#caption-synchronization" aria-label="Permalink for Section 2.2"></a></div>

<p>Captions used for accessibility purposes (also more commonly known as "subtitles" in some countries) have been in common usage in the broadcast industry for several decades. While captions for accessibility have some shared history and similarities with second language translation captions, we examine here only accessibility captions. Some of the critical accessibility issues related to captioning which have been examined in research include the caption rate, the quality of caption text (including aspects such as caption text accuracy, verbatim vs. edited captions, identification on multiple speakers, and the use of punctuation and capitalization), as well as the synchronization of caption text with audio and visual information.</p>
<section id="caption-rate"><div class="header-wrapper"><h4 id="x2-2-1-caption-rate"><bdi class="secno">2.2.1 </bdi>Caption Rate</h4><a class="self-link" href="#caption-rate" aria-label="Permalink for Section 2.2.1"></a></div>

<p>Caption rate has been a major topic for the broadcast industry. In a White Paper published by BBC Research &amp; Development [<cite><a class="bibref" data-link-type="biblio" href="#bib-display-rate" title="The impact of subtitle display rate on enjoyment under normal television viewing conditions">display-rate</a></cite>], the author summarized the various guidelines in use among broadcasters which often include both optimal and maximum rates for captions. Figures of approximately 140 Words per Minute (WPM) as the optimum subtitle (i.e., caption) rate, and around 180-200 WPM as the maximum rate were found to be common. However, the conclusion of the author was that the guidelines examined "fail to cite research supporting these figures but justify them by stating that above these rates, subtitles will be difficult to follow." Sandford further noted that previous research has shown that reading comprehension of captions remain fairly stable up to at least a rate of 230 WPM, which seemed to call into question the maximum rates used in most guidelines to that point in time, and served as the impetus for new research conducted by the BBC.</p>
<p>The BBC research study on caption rates was conducted in two phases. The first phase of the study included video clips by BBC reporters which were re-scripted as needed for the study so that they included more or less words which were spoken at a constant pace over the same 30 second period of time. In this way, a range of WPM caption rates were created while all other aspects of the clip remained the same, except that they created two types of captions, one with scrolling captions and one with block captions. This series of clips at different rates were then shown to test subjects, which included two main groups. One group of testers included deaf and hard-of-hearing viewers who viewed the video clips with captions (both scrolling and block), while a comparison group of hearing viewers viewed the same series of clips without any captions at all. The purpose of the comparison group of hearing viewers was to act as a control and help gauge how much of the impact on perceived good and bad rates of captions may be due to how quickly the speaker is talking, as opposed to how fast the words appear in captions</p>
<p>Their results for this phase of the study showed that the range of rates between what subjects considered "too fast" and "too slow" was widest for block subtitles and narrowest for speech alone. The analysis revealed that:</p>
<ul>
<li>The average rate of clips perceived as "slow" came in at 112 WPM for block captions, 115 WPM for scrolling captions, and 121 WPM for speech alone.</li>
<li>The optimal "good" rate averaged 177 WPM for block captions, 171 for scrolling captions, and 170 WPM for speech alone.</li>
<li>The average rate of clips perceived as "fast" came in at 242 for block captions, 227 for scrolling captions, and 219 for speech alone.</li>
</ul>
<p>However, the researchers concluded that overall similarity between all of the results demonstrated that the WPM rate of the caption text was not an independent factor for the study subjects' perception of rates that are too slow or too fast. Generally speaking, it was found that when the rate of speech was perceived as too fast or too slow by hearing viewers, this same range of rates for caption text was likely to be similarly perceived as too fast or too slow by viewers who were deaf and hard of hearing. Indeed, if anything, this set of data seems to suggest that--at least among the sample of subjects in this study--hearing viewers are more critically attentive to word rates in spoken audio than deaf and hard-of-hearing users are for word rates in captioned text for the same content. Overall, the researchers concluded that, "We found no problems associated with the rate of subtitles when they matched natural speech, regardless of the rate in words per minute."</p>
<p>During the second phase of the BBC study, researchers collected a number of sample clips from eight different examples of television programming "in the wild" which were above a 200 WPM rate, and presented them only to the deaf and hard-of-hearing study subjects. The expectation based on phase one of the study was that these higher rate clips would be more likely to be perceived as faster than optimal. However, the results showed that this was not the case, and that the mean perceived rates for all clips were closer to "good" and well under the "fast" rate than would have been predicted based on the findings of phase one of the study. Nonetheless, there were some telling distinctions in perception ratings based upon the type of programming. One case in point can be made by comparing ratings for two television episode clips which were nearly identical in word rate, but had a relatively wide spread in perception of how close to an optimal "enjoyable" rate, based on study subjects' numeric scores on a Likert scale. In this comparison, a clip from the factual entertainment show Top Gear with a rate of 256 WPM received a Likert scale score of 3.40 for an enjoyable rate (where 5 would be considered at the top of the "enjoyable" word rate scale), while a clip the cooking show Kitchen with an almost identical rate of 259 WPM received a lower Likert scale score of 2.34--more than a full point below the Top Gear clip, and lower than any other clip among the eight television episodes reviewed in the study.</p>
<p>While the BBC researchers in this study did not interview subjects to get additional qualitative details on subjects' ratings for individual clips, one likely conclusion is that the perception of a good or enjoyable rate for captions is tied to some extent to the type of content, and the way the viewer may plan to use the information gleaned from watching it. Whereas a typical viewer of Top Gear is likely to be more interested in the general entertainment value of watching the show, someone viewing a cooking show is more likely to be interested in actually using the information by cooking the dish being prepared on the screen. In the latter case, the viewer will often be very interested in specific details about ingredients, amounts and the cooking process. This consideration is likely very important in the context of educational video programming, in particular, where the desire is that students will comprehend and retain key facts from video programming. And although the researchers did not study these "in the wild" samples with hearing viewers, the findings from phase one of their study would logically point back to the underlying issue of speech rate in the audio stream to begin with, and suggests that media producers be careful to regulate the speed of speakers in media materials, especially when the voice content has a high information density.</p>

<p>Another study by the BBC aimed to measure the perceived quality of television captions based on guidelines for measuring perceived audio quality. The objective was to estimate the relative impact of reduced delay in the appearance of live captions vs an increase in accuracy. Participants were regular users of captions, but were not asked to disclose their hearing ability. Reduced delay in the presentation of word-by-word captions was more strongly associated with a perception of improved quality for participants watching with sound on than for those with sound turned off. On the other hand, improvement in caption accuracy (namely, a lower word error rate) was significantly associated with a perception of improved quality only among participants who viewed the material without sound [<cite><a class="bibref" data-link-type="biblio" href="#bib-live-tv-subs" title="The Development of a Methodology to Evaluate the Perceived Quality of Live TV Subtitles">live-tv-subs</a></cite>].</p>

</section>
<section id="captions-in-live-media"><div class="header-wrapper"><h4 id="x2-2-2-captions-in-live-media"><bdi class="secno">2.2.2 </bdi>Captions in Live Media</h4><a class="self-link" href="#captions-in-live-media" aria-label="Permalink for Section 2.2.2"></a></div>

<p>There are three main transcription methods for live captions. ASR (Automatic Speech Recognition), ASR with revoicing, and STTR (Speech To Text Reporting). ASR is a fully automated transcription process where a computer converts the speech into text. ASR with revoicing utilises a human intermediary who repeats everything that is spoken to an ASR system that is trained to their voice. STTR involves specially trained typists phonetically transcribing what is said using chord keyboards. The two main types of these keyboards are Palantype and Stenotype and Stenotype is the most common. Cost and accuracy both increase relative to how much of the work is done by a human.</p>
<p>Captions in live media and remote meetings will be inherently delayed due to the necessary time lag for speech to be transcribed into text captions. Even automated captions produced by ASR systems require some amount of time to process human speech into text which then must be integrated into the video stream. The use of human transcribers to create captions, while typically resulting in captions of much greater accuracy, will usually dictate an even greater latency between the sound of the speaker's voice and the displayed captions. The understanding of what is considered an "acceptable" amount of time delay will often hinge on the type of live media, and what is considered the proper level of transcription accuracy. For instance, in the case of live speeches and broadcast entertainment, media outlets have adopted caption latency standards ranging from a target as short as 3 seconds to as long as "less than 10 seconds" -- the latter case "reflecting a greater emphasis on ensuring that spelling and punctuation are correct" [<cite><a class="bibref" data-link-type="biblio" href="#bib-caption-quality" title="Caption quality: Approaches to standards and measurement">caption-quality</a></cite>]. The conclusion of Mikul is that a target latency of 5 seconds is appropriate and achievable in most cases for live broadcast media, and that this target applies to the average time lag over the length of the program.</p>
<p>However, caption time lag in remote meetings must be considered in a different light, as the participatory nature of meetings dictate that the immediacy of captioned text must take some degree of precedence over spelling and punctuation accuracy. While both accuracy and immediacy are vital criteria in any setting, having captions delayed for an inordinate amount of time during a remote meeting scenario puts the deaf or hard of hearing meeting participant at a significant disadvantage during a fast-moving discussion. To better address the need for immediacy of captions in remote meetings, most popular online meeting platforms have integrated automatic captioning utilizing Automatic Speech Recognition (ASR).</p>
<p>The accuracy of ASR can vary greatly due to the influence of factors that tend to introduce recognition errors. These factors include speaker variability, reflecting for example illness, fatigue or emotional state, differences of dialect and accent, as well as discrepancies between the audio characteristics of the speech samples used to train the system and the speech which is to be recognized, such as the presence of background noise [<cite><a class="bibref" data-link-type="biblio" href="#bib-auto-speech" title="Automatic speech recognition errors detection and correction: A review">auto-speech</a></cite>]. Nevertheless, the accuracy of ASR systems has markedly improved in recent years. Indeed, very recent studies have demonstrated that, under favorable conditions, the best ASR systems can rival human accuracy on the average while also decreasing captioning latency to well below the typical human captioning ability. For example, a 2016 MITRE Corporation study commissioned by the U.S. Federal Communications Commission to independently assess the quality metrics and associated usability of Internet Protocol Caption Telephone Service (IP CTS) found that there was at least one automatic Speech-To-Text (STT) engine that was equivalent or better than three of the four human assisted IP CTS providers studied at producing an accurate, expedient, and usable service [<cite><a class="bibref" data-link-type="biblio" href="#bib-mitre-ipcaption" title="Internet Protocol Caption Telephone Service (IP CTS) – Summary of Phase 2 Usability Testing Results">mitre-ipcaption</a></cite>]. One important viewpoint shared in the MITRE study was that users reported that they “preferred accuracy over speed as the more important variable to a successful calling experience” (page iii). A more recent 2020 study comparing ASR-based captioning systems revealed that the Google enhanced API had a stable-hypothesis latency (the time between the utterance of a word and the output of correct text) of only 0.761 seconds, while maintaining a Word Error Rate (WER) of only 0.06 [<cite><a class="bibref" data-link-type="biblio" href="#bib-cart-cc" title="A Review Of State-Of-The-Art Automatic Speech Recognition Services For CART And CC Applications">cart-cc</a></cite>]. The authors then compared this to an average latency of 4.2 seconds for human based captioning and a WER between 0.04 and 0.09, based on generalized results from multiple academic sources. While Google's enhanced ASR API was by far the best in this study comparison, its performance illustrates the growing capability of machine learning to enhance the ability of remote meeting platforms to provide accurate captions in a timely manner.</p>
  <div class="note" role="note" id="issue-container-generatedID-0"><div role="heading" class="note-title marker" id="h-note-0" aria-level="5"><span>Note</span></div><p class="">In recent years, the U.S. Federal Communications Commission has issued administrative decisions granting conditional certification for ASR generated captions in Internet telephony. See [<cite><a class="bibref" data-link-type="biblio" href="#bib-fcc-auto-speech" title="Declaratory Ruling on Automatic Speech Recognition">fcc-auto-speech</a></cite>] for the legal background to these decisions.</p></div>
</section>
	<section id="caption-synchronization-thresholds"><div class="header-wrapper"><h4 id="x2-2-3-caption-synchronization-thresholds"><bdi class="secno">2.2.3 </bdi>Caption Synchronization Thresholds</h4><a class="self-link" href="#caption-synchronization-thresholds" aria-label="Permalink for Section 2.2.3"></a></div>
	
	<p>There is need for careful consideration of the synchronization thresholds for prepared captions in prerecorded video, as well as captions in live media, live remote meetings, and meeting recordings. The following important user experience factors should be considered by authors and user agents when determining requirements for synchronization accuracy:</p>
<ul>
	<li>The caption text needs to match the timing of the speaker’s voice (and other important sounds) as closely as practical.</li> 
	<li>Caption text that appears too early can be confusing and, in some cases, can diminish the impact or enjoyment of the content, for example if the end of a joke is revealed too soon.</li>
	<li>Caption text that appears too late can add cognitive load to the viewer, especially if they can at least partially hear the sound and need to remember several seconds of audio in order to match it with the relevant captions and confirm their understanding.</li>
	<li>The caption text should avoid the appearance of the captions “flickering” due to frame synchronization issues. This can occur in prerecorded video when a caption overhangs a shot change by one frame (in either direction), which can create a distracting flicker appearance that may increase the viewer’s cognitive load.</li>
		</ul>
	<p>With these factors in mind, it is important that user agents presenting captions over the web should, at a minimum, adopt a synchronization threshold which will ensure that media captions will closely match authoring practice. It is strongly suggested that user agents should present subtitles within +/- 20ms of the authored time. In the case where lower frame rate videos are created for distribution on low data rate networks, the user agent should match the timing as referenced by the audio rather than the video. This will avoid the negative accessibility implications caused when user agents attempt to match captions to the nearest appropriate encoded video frame, which will cause temporal display errors leading to reduced display duration for a caption [<cite><a class="bibref" data-link-type="biblio" href="#bib-ebu-2018" title="TECH3380 EBU-TT-D Subtitling Distribution Format">EBU-2018</a></cite>]. Similarly, in cases where the recommended minimum frame rate cannot be maintained in video streaming and remote meeting platforms, providers should match the caption timing as referenced by the audio rather than the video to reduce temporal display errors.</p>
	</section>
</section>
<section id="sign-language-interpretation-synchronization"><div class="header-wrapper"><h3 id="x2-3-sign-language-interpretation-synchronization"><bdi class="secno">2.3 </bdi>Sign Language Interpretation Synchronization</h3><a class="self-link" href="#sign-language-interpretation-synchronization" aria-label="Permalink for Section 2.3"></a></div>

<p>While the use of closed captions in both live and prerecorded video has become widespread, the use of a human signer to provide interpretation of spoken content in media is not nearly as prevalent. In some cases, broadcasters have argued that captioning is more cost effective and reaches a larger audience of users, such as hard-of-hearing and late-deafened individuals who are not literate in sign language. However, the Deaf community has long advocated for increased availability to sign language interpretation as better meeting their access needs [<cite><a class="bibref" data-link-type="biblio" href="#bib-sign-tv" title="Sign language interpreting on TV: a reception study of visual screen exploration in deaf signing users">sign-tv</a></cite>]. And while significant research and development work has been directed toward automated sign language translation using computer-generated signing avatars, this work is still behind the current state of automated speech recognition captioning technology [<cite><a class="bibref" data-link-type="biblio" href="#bib-sign-recog" title="Sign language recognition, generation, and translation: An interdisciplinary perspective">sign-recog</a></cite>].</p>
<p>Due to the fact that sign languages have their own grammars which are not necessarily aligned to the written form of the associated spoken language, it is not possible to provide a word-by-word rendering as is done with captioning, and thus uniform synchronization with spoken audio will not be possible. Indeed, in practice a sign language interpreter will often need to wait for some few seconds to allow for an understanding of more complete spoken phrasing before starting to interpret in sign. The amount of onset time lag may vary widely depending upon the particular spoken language and the particular target sign language source.</p>
<p>In a 1983 study by Cokely, researchers found that an increased lag time actually enhanced the overall comprehension of the spoken dialogue and allowed the sign language interpreter to convey a more accurate rendering of what was spoken [<cite><a class="bibref" data-link-type="biblio" href="#bib-lag-effect" title="The effects of lag time on interpreter errors">lag-effect</a></cite>]. In this study, it was found that the number of translation errors (i.e., various types of translation miscues) decreases as the lag time of the interpreters increases. For examples, the interpreters in their study with a 2-second lag time had more than twice the total number of miscues of the interpreters with a 4-second lag, who in turn had almost twice as many miscues as those with a 6-second lag. The researchers cautioned, however, that this does not mean there is no upper limit to lag time and reasoned that it is likely there is lag time threshold beyond which the number of translation omissions would significantly increase because the threshold is at the upper limits of the individual's short-term working memory. Nonetheless, the findings of this study point out that providing close synchronization of sign language interpretation to what is being spoken may be counterproductive. In this case, some users may prefer finding a happy medium between the user need for immediacy in remote meetings and the user need for accuracy, while others may prefer the greatest accuracy possible even at the expense of immediacy.</p>
</section>

<section id="video-description-synchronization"><div class="header-wrapper"><h3 id="x2-4-video-description-synchronization"><bdi class="secno">2.4 </bdi>Video Description Synchronization</h3><a class="self-link" href="#video-description-synchronization" aria-label="Permalink for Section 2.4"></a></div>

<p>Video description (sometimes referred to as “audio description”) typically adds spoken narration of important visual elements in video streams such as TV programs and movies. Beginning in the early 1990s, the ability to transmit and receive audio descriptions in TV programming over a Separate Audio Program (SAP) channel became available [<cite><a class="bibref" data-link-type="biblio" href="#bib-dev-video" title="The Development of the Descriptive Video Services">dev-video</a></cite>]. Video description is most commonly applied to prerecorded media, although its application to live events (especially the performing arts) is also a common use case [<cite><a class="bibref" data-link-type="biblio" href="#bib-audio-live" title="Audio description for live performances and audience participation">audio-live</a></cite>].</p> 
<div class="note" role="note" id="issue-container-generatedID-1"><div role="heading" class="note-title marker" id="h-note-1" aria-level="4"><span>Note</span></div><aside class="">
  <p>Video description can be delivered by two alternative means:</p>
  <ul>
    <li>As an audio track synchronized with the visual track, or</li>
    <li>as text for presentation to the user via a text to speech system or a braille device. This is referred to in the <cite>Media Accessibility User Requirements</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-media-accessibility-reqs" title="Media Accessibility User Requirements">media-accessibility-reqs</a></cite>] as "video text description".</li>
  </ul>
  <p>Whereas the first option requires synchronization of an audio track with the video, the second option is dependent for synchronization on the user's preferences (e.g., speech rate settings). Only the start time of the cue for each description is supplied by the media provider; the end time varies according to the user's local preferences, and may necessitate automatic pausing of the video and audio tracks of the media resource to accommodate the reading of a description. See <cite>Media Accessibility User Requirements</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-media-accessibility-reqs" title="Media Accessibility User Requirements">media-accessibility-reqs</a></cite>], section 2.2, for further details.</p>
  <p>Since control of synchronization resides with the creator of the media resource if video description is provided as a supplemental audio track, only this case is considered further in the discussion that follows. The synchronization of video text description can be addressed by an appropriate implementation of client-side software that respects the user's preferences (including speech rate for text to speech systems, and scrolling behavior for braille devices).</p>
</aside></div>
<p>One of the most difficult considerations of video description creation is the need to avoid conflicts with the primary speech dialogue. In prerecorded media, a complete transcript of the spoken dialogue with timings is commonly loaded into video description editing software, although in some cases a simple spreadsheet may be used for smaller projects. The next step is typically the identification and inclusion of onscreen events, music and sound effect cues in the media time stream. Editing software may apply an algorithm that calculates the ideal duration of the description entered into the available open space time slot, as well as the minimum and maximum tolerated deviation from the ideal reading rate. In order to do that, the algorithm needs to be given reading rate values upon which it can calculate [<cite><a class="bibref" data-link-type="biblio" href="#bib-filmic-audio" title="Reading rate in filmic audio description. Rivista Internazionale di Tecnica della Traduzione">filmic-audio</a></cite>]. In practice, the description of an on-scene event may need to begin several seconds before the event occurs to avoid audio conflicts.</p>
<p>Live events, however, are more difficult to manage than prerecorded media due to the spontaneity of an event happening in real time. While video description can often be scripted during rehearsals of performances and thus made available in real time during a performance, adding video description to a live event which is not rehearsed is much more difficult because there is no pre-event information as to the availability and duration of open slots in the live audio stream. One method used to address this scenario in broadcasting of live evens is “near real-time” video description (Boyce, et al, ND). In near real-time broadcasting, the live event is recorded and transmission is typically delayed within the range of 10 to 60 seconds. This allows time for the system to look ahead and analyze the upcoming portion of the video for silent periods and provides a brief span of time for the describer to insert the narration. While near real-time video description may work well in mainstream media broadcasts, it would be impractical for online participatory events, meetings and group discussions. In such cases, the generally accepted best practice is for participants to always describe visual aspects of content or on-camera actions they may be demonstrating as closely in sync as possible with the visual information.</p> 
</section>
<section id="xr-environment-synchronization"><div class="header-wrapper"><h3 id="x2-5-xr-environment-synchronization"><bdi class="secno">2.5 </bdi>XR Environment Synchronization</h3><a class="self-link" href="#xr-environment-synchronization" aria-label="Permalink for Section 2.5"></a></div>


<p>Virtual reality and other forms of immersive digital environments are relatively new and can present a number of accessibility challenges. These challenges, and recommendations to address them, are addressed at length in the <cite>XR Accessibility User Requirements</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-xaur" title="XR Accessibility User Requirements">xaur</a></cite>]. It is not the purpose of this document to do that. We address here only those matters which relate to synchronization. It is to be expected that our understanding of this area will evolve considerably once these scenarios have become the subject of further research.</p>
<p>One aspect of note here, is that both captioning of speech and video description (aka, “audio description”) of visual and auditory information presented in immersive environments may need to have timing and synchronization adjustments to provide an inclusive experience. For instance, in virtual reality environments or 360-degree audiovisual presentations, co-participants or important auditory events may appear behind you. In such cases, some accessible alert mechanisms are needed to alert deaf users that an action is happening outside their vision (i.e., behind them). This should be synchronized with the actions they refer to. To support this user need, this would necessitate that captions should remain in place longer than is customary for standard 2-dimensional video. Further, since 360-degree environments may often have changing depths, this longer than usual display time would also help allow one’s eyes to refocus when moving the user’s gaze to various objects which appear at different distances from the observer.</p>

<div class="note" id="issue-container-generatedID-2"><div role="heading" class="ednote-title marker" id="h-ednote" aria-level="4"><span>Editor's note</span></div><p class="">The Task Force welcomes references to any research investigating synchronization issues specific to immersive environments, as well as more general comments on this section.</p></div>
</section>
</section>
		
<section class="appendix" id="acknowledgements"><div class="header-wrapper"><h2 id="a-acknowledgements"><bdi class="secno">A. </bdi>Acknowledgements</h2><a class="self-link" href="#acknowledgements" aria-label="Permalink for Appendix A."></a></div>
  
    <section id="enabling-funders"><div class="header-wrapper"><h3 id="a-1-enabling-funders"><bdi class="secno">A.1 </bdi>Enabling Funders</h3><a class="self-link" href="#enabling-funders" aria-label="Permalink for Appendix A.1"></a></div>
      
      <p>This work is supported by the <a href="https://www.w3.org/WAI/about/projects/wai-guide/">EC-funded WAI-Guide Project</a>. This work has also been funded initially under contract number ED-OSE-10-C-0067, then under contract number HHSP23301500054C, and now under HHS75P00120P00168. The content of this publication does not necessarily reflect the views or policies of the U.S. Department of Health and Human Services, nor does mention of trade names, commercial products, or organizations imply endorsement by the U.S. Government.</p>
  </section>
</section>

<script>
	// <![CDATA[  <-- For SVG support
	if ('WebSocket' in window) {
		(function () {
			function refreshCSS() {
				var sheets = [].slice.call(document.getElementsByTagName("link"));
				var head = document.getElementsByTagName("head")[0];
				for (var i = 0; i < sheets.length; ++i) {
					var elem = sheets[i];
					var parent = elem.parentElement || head;
					parent.removeChild(elem);
					var rel = elem.rel;
					if (elem.href && typeof rel != "string" || rel.length == 0 || rel.toLowerCase() == "stylesheet") {
						var url = elem.href.replace(/(&|\?)_cacheOverride=\d+/, '');
						elem.href = url + (url.indexOf('?') >= 0 ? '&' : '?') + '_cacheOverride=' + (new Date().valueOf());
					}
					parent.appendChild(elem);
				}
			}
			var protocol = window.location.protocol === 'http:' ? 'ws://' : 'wss://';
			var address = protocol + window.location.host + window.location.pathname + '/ws';
			var socket = new WebSocket(address);
			socket.onmessage = function (msg) {
				if (msg.data == 'reload') window.location.reload();
				else if (msg.data == 'refreshcss') refreshCSS();
			};
			if (sessionStorage && !sessionStorage.getItem('IsThisFirstTime_Log_From_LiveServer')) {
				console.log('Live reload enabled.');
				sessionStorage.setItem('IsThisFirstTime_Log_From_LiveServer', true);
			}
		})();
	}
	else {
		console.error('Upgrade your browser. This Browser is NOT supported WebSocket for Live-Reloading.');
	}
	// ]]>
</script>


<section id="references" class="appendix"><div class="header-wrapper"><h2 id="b-references"><bdi class="secno">B. </bdi>References</h2><a class="self-link" href="#references" aria-label="Permalink for Appendix B."></a></div><section id="informative-references"><div class="header-wrapper"><h3 id="b-1-informative-references"><bdi class="secno">B.1 </bdi>Informative references</h3><a class="self-link" href="#informative-references" aria-label="Permalink for Appendix B.1"></a></div>
    
    <dl class="bibliography"><dt id="bib-audio-live">[audio-live]</dt><dd>
      <cite>Audio description for live performances and audience participation</cite>. Di Giovanni, E. 2018. 
    </dd><dt id="bib-auto-speech">[auto-speech]</dt><dd>
      <cite>Automatic speech recognition errors detection and correction: A review</cite>. Errattahi, R.; El Hannani, A.; Ouahmane, H. 2018. 
    </dd><dt id="bib-caption-quality">[caption-quality]</dt><dd>
      <cite>Caption quality: Approaches to standards and measurement</cite>. Mikul, C. 2014. 
    </dd><dt id="bib-cart-cc">[cart-cc]</dt><dd>
      <cite>A Review Of State-Of-The-Art Automatic Speech Recognition Services For CART And CC Applications</cite>. Jiline, M; Kirk, D.; Quirk, K; Sandler, M.; Monette, M. 2020. 
    </dd><dt id="bib-dev-video">[dev-video]</dt><dd>
      <cite>The Development of the Descriptive Video Services</cite>. Cronin, B. J.; King, S. R. 1990. 
    </dd><dt id="bib-display-rate">[display-rate]</dt><dd>
      <cite>The impact of subtitle display rate on enjoyment under normal television viewing conditions</cite>. Sandford, J.. 2015. 
    </dd><dt id="bib-distance-effects">[distance-effects]</dt><dd>
      <cite>Effects of distance on visual and audiovisual speech recognition</cite>. Jordan, T. R; Sergeant, P.. 2000. 
    </dd><dt id="bib-ebu-2018">[EBU-2018]</dt><dd>
      <a href="https://tech.ebu.ch/publications/tech3380"><cite>TECH3380 EBU-TT-D Subtitling Distribution Format</cite></a>.  EBU. 22 May 2018. URL: <a href="https://tech.ebu.ch/publications/tech3380">https://tech.ebu.ch/publications/tech3380</a>
    </dd><dt id="bib-en-301-549">[en-301-549]</dt><dd>
      <a href="https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf"><cite>EN 301 549 v3.2.1: Harmonised European Standard - Accessibility requirements for ICT products and services</cite></a>.  CEN/CENELEC/ETSI. 2021-03. URL: <a href="https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf">https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf</a>
    </dd><dt id="bib-fcc-auto-speech">[fcc-auto-speech]</dt><dd>
      <cite>Declaratory Ruling on Automatic Speech Recognition</cite>. Federal Communications Commission. 2018. 
    </dd><dt id="bib-filmic-audio">[filmic-audio]</dt><dd>
      <cite>Reading rate in filmic audio description. Rivista Internazionale di Tecnica della Traduzione</cite>. Jankowska, A.; ZióŁko, B.; Igras-Cybulska, M; Psiuk, A.. 2017. 
    </dd><dt id="bib-gaze-patterns">[gaze-patterns]</dt><dd>
      <cite>Gaze patterns and audiovisual speech enhancement</cite>. Yi, A; Wong, W.; Eizenman, M. 2013. 
    </dd><dt id="bib-integration-multi">[integration-multi]</dt><dd>
      <cite>Audio-visual integration in multimodal communication</cite>. Chen, T.; Rao, R. R.. 1998. 
    </dd><dt id="bib-lag-effect">[lag-effect]</dt><dd>
      <cite>The effects of lag time on interpreter errors</cite>. Cokely, D. 1986. 
    </dd><dt id="bib-lip-speech">[lip-speech]</dt><dd>
      <cite>Lipreading and audio-visual speech perception</cite>. Summerfield, Q. 1992. 
    </dd><dt id="bib-lip-sync-video">[lip-sync-video]</dt><dd>
      <cite>Lip Synchronization in Video Conferencing</cite>. Firestone, S. 2007. 
    </dd><dt id="bib-live-tv-subs">[live-tv-subs]</dt><dd>
      <cite>The Development of a Methodology to Evaluate the Perceived Quality of Live TV Subtitles</cite>. Armstrong, M.. Oct 2013. 
    </dd><dt id="bib-media-accessibility-reqs">[media-accessibility-reqs]</dt><dd>
      <a href="https://www.w3.org/TR/media-accessibility-reqs/"><cite>Media Accessibility User Requirements</cite></a>. Shane McCarron; Michael Cooper; Mark Sadecki.  W3C. 3 December 2015. URL: <a href="https://www.w3.org/TR/media-accessibility-reqs/">https://www.w3.org/TR/media-accessibility-reqs/</a>
    </dd><dt id="bib-mitre-ipcaption">[mitre-ipcaption]</dt><dd>
      <a href="https://ecfsapi.fcc.gov/file/10411287298464/MITRE%20Corporation%20Summary%20of%20Phase%202.pdf"><cite>Internet Protocol Caption Telephone Service (IP CTS) – Summary of Phase 2 Usability Testing Results</cite></a>.  MITRE Corporation. 23 March 2016. URL: <a href="https://ecfsapi.fcc.gov/file/10411287298464/MITRE%20Corporation%20Summary%20of%20Phase%202.pdf">https://ecfsapi.fcc.gov/file/10411287298464/MITRE%20Corporation%20Summary%20of%20Phase%202.pdf</a>
    </dd><dt id="bib-multimodal-speech">[multimodal-speech]</dt><dd>
      <cite>Multimodal speech recognition: increasing accuracy using high speed video data</cite>. Ivanko, D.; Karpov, A.; Fedotov, D.; Kipyatkova, I.; Ryumin, D.; Zelezny, M. 2018. 
    </dd><dt id="bib-neural-synchrony">[neural-synchrony]</dt><dd>
      <cite>Enhanced neural synchrony between left auditory and premotor cortex is associated with successful phonetic categorization.</cite>. Alho, J; Lin, F; H., Sato; M., Tiitinen; H., Sams; M., &amp; Jääskeläinen. 2014. 
    </dd><dt id="bib-prediction-const">[prediction-const]</dt><dd>
      <cite>Prediction and constraint in audiovisual speech perception</cite>. Peelle, J. E.; Sommers, M. S. 2015. 
    </dd><dt id="bib-raur">[raur]</dt><dd>
      <a href="https://www.w3.org/TR/raur/"><cite>RTC Accessibility User Requirements</cite></a>. Joshue O'Connor; Janina Sajka; Jason White; Michael Cooper.  W3C. 25 May 2021. URL: <a href="https://www.w3.org/TR/raur/">https://www.w3.org/TR/raur/</a>
    </dd><dt id="bib-sign-recog">[sign-recog]</dt><dd>
      <cite>Sign language recognition, generation, and translation: An interdisciplinary perspective</cite>. Bragg, D; Koller, O.; Bellard, M.; Berke, L; Boudreault, P.; Braffort, A.; Ringel Morris, M. 2019. 
    </dd><dt id="bib-sign-tv">[sign-tv]</dt><dd>
      <cite>Sign language interpreting on TV: a reception study of visual screen exploration in deaf signing users</cite>. Bosch-Baliarda, Marta; Olga Soler-Vilageliu; Pilar Orero. 2020. 
    </dd><dt id="bib-simul-trans-video">[simul-trans-video]</dt><dd>
      <cite>Assessing the importance of audio/video synchronization for simultaneous translation of video sequences</cite>. Staelens; Nicolas &amp; De Meulenaere; Jonas &amp; Bleumers; Lizzy &amp; Wallendael et al. 2012. 
    </dd><dt id="bib-speech-intel">[speech-intel]</dt><dd>
      <cite>Speech intelligibility derived from asynchronous processing of auditory-visual information</cite>. Grant, K. W.; Greenberg, S. 2001. 
    </dd><dt id="bib-speech-intel-noise">[speech-intel-noise]</dt><dd>
      <cite>Visual contribution to speech intelligibility in noise</cite>. Sumby, W. H; Pollack, I. 1954. 
    </dd><dt id="bib-wcag21">[wcag21]</dt><dd>
      <a href="https://www.w3.org/TR/WCAG21/"><cite>Web Content Accessibility Guidelines (WCAG) 2.1</cite></a>. Andrew Kirkpatrick; Joshue O'Connor; Alastair Campbell; Michael Cooper.  W3C. 5 June 2018. URL: <a href="https://www.w3.org/TR/WCAG21/">https://www.w3.org/TR/WCAG21/</a>
    </dd><dt id="bib-xaur">[xaur]</dt><dd>
      <a href="https://www.w3.org/TR/xaur/"><cite>XR Accessibility User Requirements</cite></a>. Joshue O'Connor; Janina Sajka; Jason White; Scott Hollier; Michael Cooper.  W3C. 16 Sept 2020. URL: <a href="https://www.w3.org/TR/xaur/">https://www.w3.org/TR/xaur/</a>
    </dd></dl>
  </section></section><p role="navigation" id="back-to-top">
    <a href="#title"><abbr title="Back to Top">↑</abbr></a>
  </p><script id="respec-dfn-panel">(() => {
// @ts-check
if (document.respec) {
  document.respec.ready.then(setupPanel);
} else {
  setupPanel();
}

function setupPanel() {
  const listener = panelListener();
  document.body.addEventListener("keydown", listener);
  document.body.addEventListener("click", listener);
}

function panelListener() {
  /** @type {HTMLElement} */
  let panel = null;
  return event => {
    const { target, type } = event;

    if (!(target instanceof HTMLElement)) return;

    // For keys, we only care about Enter key to activate the panel
    // otherwise it's activated via a click.
    if (type === "keydown" && event.key !== "Enter") return;

    const action = deriveAction(event);

    switch (action) {
      case "show": {
        hidePanel(panel);
        /** @type {HTMLElement} */
        const dfn = target.closest("dfn, .index-term");
        panel = document.getElementById(`dfn-panel-for-${dfn.id}`);
        const coords = deriveCoordinates(event);
        displayPanel(dfn, panel, coords);
        break;
      }
      case "dock": {
        panel.style.left = null;
        panel.style.top = null;
        panel.classList.add("docked");
        break;
      }
      case "hide": {
        hidePanel(panel);
        panel = null;
        break;
      }
    }
  };
}

/**
 * @param {MouseEvent|KeyboardEvent} event
 */
function deriveCoordinates(event) {
  const target = /** @type HTMLElement */ (event.target);

  // We prevent synthetic AT clicks from putting
  // the dialog in a weird place. The AT events sometimes
  // lack coordinates, so they have clientX/Y = 0
  const rect = target.getBoundingClientRect();
  if (
    event instanceof MouseEvent &&
    event.clientX >= rect.left &&
    event.clientY >= rect.top
  ) {
    // The event probably happened inside the bounding rect...
    return { x: event.clientX, y: event.clientY };
  }

  // Offset to the middle of the element
  const x = rect.x + rect.width / 2;
  // Placed at the bottom of the element
  const y = rect.y + rect.height;
  return { x, y };
}

/**
 * @param {Event} event
 */
function deriveAction(event) {
  const target = /** @type {HTMLElement} */ (event.target);
  const hitALink = !!target.closest("a");
  if (target.closest("dfn:not([data-cite]), .index-term")) {
    return hitALink ? "none" : "show";
  }
  if (target.closest(".dfn-panel")) {
    if (hitALink) {
      return target.classList.contains("self-link") ? "hide" : "dock";
    }
    const panel = target.closest(".dfn-panel");
    return panel.classList.contains("docked") ? "hide" : "none";
  }
  if (document.querySelector(".dfn-panel:not([hidden])")) {
    return "hide";
  }
  return "none";
}

/**
 * @param {HTMLElement} dfn
 * @param {HTMLElement} panel
 * @param {{ x: number, y: number }} clickPosition
 */
function displayPanel(dfn, panel, { x, y }) {
  panel.hidden = false;
  // distance (px) between edge of panel and the pointing triangle (caret)
  const MARGIN = 20;

  const dfnRects = dfn.getClientRects();
  // Find the `top` offset when the `dfn` can be spread across multiple lines
  let closestTop = 0;
  let minDiff = Infinity;
  for (const rect of dfnRects) {
    const { top, bottom } = rect;
    const diffFromClickY = Math.abs((top + bottom) / 2 - y);
    if (diffFromClickY < minDiff) {
      minDiff = diffFromClickY;
      closestTop = top;
    }
  }

  const top = window.scrollY + closestTop + dfnRects[0].height;
  const left = x - MARGIN;
  panel.style.left = `${left}px`;
  panel.style.top = `${top}px`;

  // Find if the panel is flowing out of the window
  const panelRect = panel.getBoundingClientRect();
  const SCREEN_WIDTH = Math.min(window.innerWidth, window.screen.width);
  if (panelRect.right > SCREEN_WIDTH) {
    const newLeft = Math.max(MARGIN, x + MARGIN - panelRect.width);
    const newCaretOffset = left - newLeft;
    panel.style.left = `${newLeft}px`;
    /** @type {HTMLElement} */
    const caret = panel.querySelector(".caret");
    caret.style.left = `${newCaretOffset}px`;
  }

  // As it's a dialog, we trap focus.
  // TODO: when <dialog> becomes a implemented, we should really
  // use that.
  trapFocus(panel, dfn);
}

/**
 * @param {HTMLElement} panel
 * @param {HTMLElement} dfn
 * @returns
 */
function trapFocus(panel, dfn) {
  /** @type NodeListOf<HTMLAnchorElement> elements */
  const anchors = panel.querySelectorAll("a[href]");
  // No need to trap focus
  if (!anchors.length) return;

  // Move focus to first anchor element
  const first = anchors.item(0);
  first.focus();

  const trapListener = createTrapListener(anchors, panel, dfn);
  panel.addEventListener("keydown", trapListener);

  // Hiding the panel releases the trap
  const mo = new MutationObserver(records => {
    const [record] = records;
    const target = /** @type HTMLElement */ (record.target);
    if (target.hidden) {
      panel.removeEventListener("keydown", trapListener);
      mo.disconnect();
    }
  });
  mo.observe(panel, { attributes: true, attributeFilter: ["hidden"] });
}

/**
 *
 * @param {NodeListOf<HTMLAnchorElement>} anchors
 * @param {HTMLElement} panel
 * @param {HTMLElement} dfn
 * @returns
 */
function createTrapListener(anchors, panel, dfn) {
  const lastIndex = anchors.length - 1;
  let currentIndex = 0;
  return event => {
    switch (event.key) {
      // Hitting "Tab" traps us in a nice loop around elements.
      case "Tab": {
        event.preventDefault();
        currentIndex += event.shiftKey ? -1 : +1;
        if (currentIndex < 0) {
          currentIndex = lastIndex;
        } else if (currentIndex > lastIndex) {
          currentIndex = 0;
        }
        anchors.item(currentIndex).focus();
        break;
      }

      // Hitting "Enter" on an anchor releases the trap.
      case "Enter":
        hidePanel(panel);
        break;

      // Hitting "Escape" returns focus to dfn.
      case "Escape":
        hidePanel(panel);
        dfn.focus();
        return;
    }
  };
}

/** @param {HTMLElement} panel */
function hidePanel(panel) {
  if (!panel) return;
  panel.hidden = true;
  panel.classList.remove("docked");
}
})()</script><script src="https://www.w3.org/scripts/TR/2021/fixup.js"></script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>